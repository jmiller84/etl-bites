{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbname='etl_bites' user='joemiller'\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import os\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv #pip install python-dotenv\n",
    "from psycopg2 import connect, sql\n",
    "from os import environ as env\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "conn_string = os.getenv('conn_string')\n",
    "AWS_ACCESS_KEY = os.getenv('AWS_ACCESS_KEY')\n",
    "AWS_SECRET_KEY = os.getenv('AWS_SECRET_KEY')\n",
    "\n",
    "\n",
    "\n",
    "if 'conn_string' in env:\n",
    "    print(env['conn_string'][:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Large Cloud Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT\n",
    "\n",
    "We are using AWS Athena to query a large dataset stored in an AWS S3 Bucket.\n",
    "\n",
    "This query would require too much compute power for our local machine se we do the large query in the cloud using Athena.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "# Set up the Athena client\n",
    "athena_client = boto3.client(\n",
    "    'athena',\n",
    "    region_name='eu-west-2',\n",
    "    aws_access_key_id= AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key= AWS_SECRET_KEY,\n",
    "    )\n",
    "\n",
    "# Write the SQL query\n",
    "sql_query = \"\"\"\n",
    "    SELECT server_request_country_code, COUNT(*) as total_visits\n",
    "    FROM vod_clickstream\n",
    "    WHERE datetime >= CAST('2018-01-01' AS timestamp) AND datetime < CAST('2018-02-01' AS timestamp)\n",
    "    GROUP BY server_request_country_code;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Execute the Athena query\n",
    "query_execution = athena_client.start_query_execution(\n",
    "    QueryString=sql_query,\n",
    "    QueryExecutionContext={\n",
    "        \"Database\": \"joe-athena_parquet\"\n",
    "    },\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": \"s3://athena-learners-etl-bite05/joe\" # <= This will be different for you, refer to the Amazon Athena pill for more information.\n",
    "    }\n",
    ")\n",
    "\n",
    "# Poll the query status until it is either successful or failed\n",
    "query_status = \"QUEUED\"\n",
    "query_execution_id = query_execution[\"QueryExecutionId\"]\n",
    "\n",
    "while query_status in [\"QUEUED\", \"RUNNING\"]:\n",
    "    query_execution = athena_client.get_query_execution(\n",
    "        QueryExecutionId=query_execution_id\n",
    "    )\n",
    "    query_status = query_execution[\"QueryExecution\"][\"Status\"][\"State\"]\n",
    "    if query_status == \"FAILED\":\n",
    "        raise Exception(\"Athena query failed!\")\n",
    "    time.sleep(1)\n",
    "\n",
    "# Retrieve the query results\n",
    "results = athena_client.get_query_results(\n",
    "    QueryExecutionId=query_execution_id\n",
    ")[\"ResultSet\"][\"Rows\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSFORM/LOAD\n",
    "\n",
    "- We will then save the result of our Athena query locally using Postgres, where we can transform and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row: {'Data': [{}, {'VarCharValue': '243775'}]}\n"
     ]
    }
   ],
   "source": [
    "# Connect to the local Postgres database\n",
    "conn = psycopg2.connect(conn_string)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the table if it doesn't exist\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS visits_per_country (\n",
    "        country_code VARCHAR(2) PRIMARY KEY,\n",
    "        total_visits INTEGER\n",
    "    );\n",
    "\"\"\")\n",
    "            \n",
    "# Process the query results\n",
    "for row in results[1:]:\n",
    "    if not row[\"Data\"][0] or not row[\"Data\"][1]:\n",
    "        # You could also print what a `row` has if you are curious!\n",
    "        print(f\"Skipping row: {row}\")\n",
    "        continue\n",
    "\n",
    "    country_code = row[\"Data\"][0][\"VarCharValue\"]\n",
    "    total_visits = int(row[\"Data\"][1][\"VarCharValue\"])\n",
    "\n",
    "    # Insert the data into the local PostgreSQL database\n",
    "    insert_query = \"\"\"\n",
    "        INSERT INTO visits_per_country (country_code, total_visits)\n",
    "        VALUES (%s, %s)\n",
    "        ON CONFLICT (country_code)\n",
    "        DO UPDATE SET total_visits = EXCLUDED.total_visits;\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        cursor.execute(insert_query, (country_code, total_visits))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error occurred inserting into analytical DB: %s\"% e)\n",
    "        conn.rollback()  # Rollback the transaction if there's an error\n",
    "\n",
    "# Commit the changes and close the cursor and connection outside the loop\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etl-bites-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
